#!/usr/bin/env python3
"""
AkakÃ§e Entegre Scraper - Dashboard Ä°Ã§in
GerÃ§ek AkakÃ§e verilerini Ã§ekip veritabanÄ±na kaydeder
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import json
import sys
import os
from datetime import datetime, timedelta
import logging
from urllib.parse import quote_plus
import re

# Proje dizinini path'e ekle
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.database.models import get_db_session, Product, MarketPrice, PriceAnomaly

# Logging yapÄ±landÄ±rmasÄ±
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AkakceScraper:
    def __init__(self):
        """AkakÃ§e scraper baÅŸlatÄ±cÄ±sÄ±"""
        self.session = requests.Session()
        self.setup_session()
        self.anomaly_threshold = 10.0  # %10 fark anomali sayÄ±lÄ±r
        
    def setup_session(self):
        """HTTP session ayarlarÄ±"""
        self.session.headers.update({
            'User-Agent': random.choice([
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            ]),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
    def search_product(self, product_name):
        """AkakÃ§e'de Ã¼rÃ¼n ara"""
        try:
            # URL encode
            search_query = quote_plus(product_name)
            search_url = f"https://www.akakce.com/arama/?q={search_query}"
            
            logger.info(f"AkakÃ§e'de aranÄ±yor: {product_name}")
            
            # Ä°stek gÃ¶nder
            response = self.session.get(search_url, timeout=15)
            response.raise_for_status()
            
            # Parse HTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ÃœrÃ¼n linklerini bul
            product_links = []
            
            # Ana Ã¼rÃ¼n listesi
            product_elements = soup.find_all('div', class_='p')
            for element in product_elements[:3]:  # Ä°lk 3 sonuÃ§
                link_elem = element.find('a')
                if link_elem and link_elem.get('href'):
                    product_url = 'https://www.akakce.com' + link_elem['href']
                    product_title = link_elem.get_text(strip=True)
                    product_links.append({
                        'url': product_url,
                        'title': product_title
                    })
            
            if not product_links:
                # Alternatif arama
                product_elements = soup.find_all('a', href=re.compile(r'/[^/]+\.html'))
                for element in product_elements[:3]:
                    if element.get('href') and element.get_text(strip=True):
                        product_url = 'https://www.akakce.com' + element['href']
                        product_title = element.get_text(strip=True)
                        if len(product_title) > 10 and product_name.lower().split()[0] in product_title.lower():
                            product_links.append({
                                'url': product_url,
                                'title': product_title
                            })
            
            logger.info(f"{len(product_links)} Ã¼rÃ¼n bulundu")
            return product_links
            
        except Exception as e:
            logger.error(f"ÃœrÃ¼n arama hatasÄ±: {e}")
            return []
    
    def get_product_prices(self, product_url):
        """ÃœrÃ¼n sayfasÄ±ndan fiyatlarÄ± Ã§ek"""
        try:
            logger.info(f"Fiyatlar Ã§ekiliyor: {product_url}")
            
            response = self.session.get(product_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            prices = []
            
            # Debug: Sayfa iÃ§eriÄŸini kontrol et
            logger.debug(f"Sayfa baÅŸlÄ±ÄŸÄ±: {soup.title.string if soup.title else 'Yok'}")
            
            # Modern AkakÃ§e fiyat yapÄ±sÄ±nÄ± ara
            # 1. Yeni tablo yapÄ±sÄ±
            price_table = soup.find('table', class_='w')
            if price_table:
                rows = price_table.find_all('tr')[1:]  # Ä°lk satÄ±r baÅŸlÄ±k
                for row in rows[:10]:
                    try:
                        # SatÄ±cÄ± adÄ± (genellikle ikinci td)
                        cells = row.find_all('td')
                        if len(cells) < 2:
                            continue
                            
                        merchant_cell = cells[1] if len(cells) > 1 else cells[0]
                        merchant_name = merchant_cell.get_text(strip=True)
                        
                        # Fiyat - farklÄ± olasÄ± pozisyonlar
                        price_text = None
                        for cell in cells:
                            text = cell.get_text(strip=True)
                            if 'â‚º' in text or 'TL' in text:
                                price_text = text
                                break
                        
                        if not price_text:
                            continue
                            
                        # FiyatÄ± temizle ve Ã§evir
                        price_clean = re.sub(r'[^\d,.]', '', price_text)
                        price_clean = price_clean.replace('.', '').replace(',', '.')
                        
                        try:
                            price = float(price_clean)
                            if price > 100:  # Ã‡ok dÃ¼ÅŸÃ¼k fiyatlarÄ± filtrele
                                prices.append({
                                    'merchant': merchant_name[:50],  # Uzun isimleri kÄ±salt
                                    'price': price,
                                    'currency': 'TRY'
                                })
                        except ValueError:
                            continue
                            
                    except Exception as e:
                        logger.debug(f"SatÄ±r parse hatasÄ±: {e}")
                        continue
            
            # 2. Alternatif: TÃ¼m fiyat patternlerini ara
            if not prices:
                price_patterns = [
                    r'(\d{1,3}(?:\.\d{3})*(?:,\d{2})?)\s*â‚º',
                    r'(\d{1,3}(?:\.\d{3})*(?:,\d{2})?)\s*TL',
                    r'â‚º\s*(\d{1,3}(?:\.\d{3})*(?:,\d{2})?)',
                ]
                
                page_text = soup.get_text()
                for pattern in price_patterns:
                    matches = re.findall(pattern, page_text)
                    for match in matches[:5]:
                        try:
                            price_clean = match.replace('.', '').replace(',', '.')
                            price = float(price_clean)
                            if 1000 < price < 200000:  # Makul fiyat aralÄ±ÄŸÄ±
                                prices.append({
                                    'merchant': 'AkakÃ§e Fiyat',
                                    'price': price,
                                    'currency': 'TRY'
                                })
                        except ValueError:
                            continue
                    if prices:
                        break
            
            # 3. Son alternatif: TÃ¼m sayÄ±sal deÄŸerleri kontrol et
            if not prices:
                all_numbers = re.findall(r'\d{2,6}', soup.get_text())
                for num_str in all_numbers[:10]:
                    try:
                        num = float(num_str)
                        if 10000 < num < 100000:  # iPhone fiyat aralÄ±ÄŸÄ±
                            prices.append({
                                'merchant': 'AkakÃ§e Tahmin',
                                'price': num,
                                'currency': 'TRY'
                            })
                    except ValueError:
                        continue
                    if len(prices) >= 3:
                        break
            
            # FiyatlarÄ± temizle ve sÄ±rala
            if prices:
                # DuplikatlarÄ± kaldÄ±r
                unique_prices = []
                seen_prices = set()
                for p in prices:
                    price_key = (p['merchant'], int(p['price']))
                    if price_key not in seen_prices:
                        unique_prices.append(p)
                        seen_prices.add(price_key)
                
                prices = sorted(unique_prices, key=lambda x: x['price'])[:8]
            
            logger.info(f"{len(prices)} fiyat bulundu")
            return prices
            
        except Exception as e:
            logger.error(f"Fiyat Ã§ekme hatasÄ±: {e}")
            return []
    
    def scrape_product_data(self, product_name):
        """ÃœrÃ¼n iÃ§in tÃ¼m veriyi Ã§ek"""
        try:
            # Rastgele gecikme
            time.sleep(random.uniform(2, 5))
            
            # ÃœrÃ¼n ara
            product_links = self.search_product(product_name)
            if not product_links:
                logger.warning(f"ÃœrÃ¼n bulunamadÄ±: {product_name}")
                return None
            
            all_prices = []
            
            # Her Ã¼rÃ¼n linkinden fiyat Ã§ek
            for product_link in product_links[:2]:  # Ä°lk 2 Ã¼rÃ¼n
                prices = self.get_product_prices(product_link['url'])
                for price_data in prices:
                    price_data['product_title'] = product_link['title']
                    price_data['product_url'] = product_link['url']
                all_prices.extend(prices)
                
                # SaygÄ±lÄ± gecikme
                time.sleep(random.uniform(1, 3))
            
            if not all_prices:
                logger.warning(f"Fiyat bulunamadÄ±: {product_name}")
                return None
            
            # Ä°statistikleri hesapla
            price_values = [p['price'] for p in all_prices]
            
            result = {
                'product_name': product_name,
                'scraped_at': datetime.now(),
                'prices': all_prices,
                'min_price': min(price_values),
                'max_price': max(price_values),
                'avg_price': sum(price_values) / len(price_values),
                'median_price': sorted(price_values)[len(price_values)//2],
                'price_count': len(price_values)
            }
            
            logger.info(f"âœ… {product_name}: {len(price_values)} fiyat - Ort: â‚º{result['avg_price']:.2f}")
            return result
            
        except Exception as e:
            logger.error(f"Scraping hatasÄ± {product_name}: {e}")
            return None
    
    def save_to_database(self, product_id, scraped_data):
        """Ã‡ekilen veriyi veritabanÄ±na kaydet"""
        if not scraped_data:
            return False
        
        try:
            session = get_db_session()
            
            # Her fiyat iÃ§in MarketPrice kaydÄ± oluÅŸtur
            for price_data in scraped_data['prices']:
                market_price = MarketPrice(
                    product_id=product_id,
                    source='akakce',
                    seller_name=price_data['merchant'],
                    price=price_data['price'],
                    currency=price_data.get('currency', 'TRY'),
                    scraped_at=scraped_data['scraped_at']
                )
                session.add(market_price)
            
            session.commit()
            session.close()
            
            logger.info(f"âœ… VeritabanÄ±na kaydedildi: {scraped_data['product_name']}")
            return True
            
        except Exception as e:
            logger.error(f"VeritabanÄ± kayÄ±t hatasÄ±: {e}")
            return False
    
    def detect_price_anomalies(self, product_id, our_price, market_data):
        """Fiyat anomalilerini tespit et"""
        if not market_data:
            logger.info("Market data yok, anomali tespiti yapÄ±lamÄ±yor")
            return []
        
        anomalies = []
        session = get_db_session()
        
        try:
            market_avg = market_data['avg_price']
            
            # Fiyat farkÄ±nÄ± hesapla
            price_diff_percent = ((our_price - market_avg) / market_avg) * 100
            
            logger.info(f"ğŸ” Anomali kontrolÃ¼: Bizim=â‚º{our_price}, Piyasa=â‚º{market_avg:.2f}, Fark=%{price_diff_percent:.1f}")
            logger.info(f"ğŸ” Threshold: %{self.anomaly_threshold}")
            
            # Anomali kontrolÃ¼
            if abs(price_diff_percent) > self.anomaly_threshold:
                
                # Anomali tÃ¼rÃ¼nÃ¼ belirle
                if price_diff_percent > 0:
                    anomaly_type = 'price_high'
                else:
                    anomaly_type = 'price_low'
                
                # Ã–nem derecesi
                if abs(price_diff_percent) > 25:
                    severity = 'high'
                elif abs(price_diff_percent) > 15:
                    severity = 'medium'
                else:
                    severity = 'low'
                
                logger.info(f"âš ï¸ Anomali tespit edildi! Tip: {anomaly_type}, Ã–nem: {severity}")
                
                # Anomali kaydÄ± oluÅŸtur
                anomaly = PriceAnomaly(
                    product_id=product_id,
                    anomaly_type=anomaly_type,
                    severity=severity,
                    deviation_percent=price_diff_percent,
                    detected_at=datetime.now(),
                    is_resolved=False,
                    notes=f"AkakÃ§e scraping ile tespit edildi - {datetime.now().strftime('%d.%m.%Y %H:%M')}"
                )
                
                session.add(anomaly)
                session.flush()  # ID almak iÃ§in
                logger.info(f"âœ… Anomali veritabanÄ±na eklendi: ID={anomaly.id}")
                
                anomalies.append({
                    'type': anomaly_type,
                    'severity': severity,
                    'deviation': price_diff_percent,
                    'our_price': our_price,
                    'market_avg': market_avg
                })
            else:
                logger.info(f"â„¹ï¸ Anomali yok: Fark %{abs(price_diff_percent):.1f} < threshold %{self.anomaly_threshold}")
            
            session.commit()
            logger.info(f"ğŸ’¾ Database commit yapÄ±ldÄ±")
            session.close()
            
            return anomalies
            
        except Exception as e:
            logger.error(f"Anomali tespit hatasÄ±: {e}")
            import traceback
            logger.error(traceback.format_exc())
            session.rollback()
            session.close()
            return []
    
    def scrape_all_products(self):
        """VeritabanÄ±ndaki tÃ¼m aktif Ã¼rÃ¼nler iÃ§in scraping yap"""
        session = get_db_session()
        
        try:
            # Aktif Ã¼rÃ¼nleri al
            products = session.query(Product).filter(Product.is_active == True).all()
            session.close()
            
            logger.info(f"ğŸš€ {len(products)} Ã¼rÃ¼n iÃ§in AkakÃ§e scraping baÅŸlÄ±yor...")
            
            scraped_count = 0
            anomaly_count = 0
            
            for product in products:
                try:
                    logger.info(f"ğŸ“Š Ä°ÅŸleniyor: {product.name}")
                    
                    # ÃœrÃ¼n iÃ§in AkakÃ§e'den veri Ã§ek
                    scraped_data = self.scrape_product_data(product.name)
                    
                    if scraped_data:
                        # VeritabanÄ±na kaydet
                        if self.save_to_database(product.id, scraped_data):
                            scraped_count += 1
                        
                        # Anomali tespiti
                        anomalies = self.detect_price_anomalies(
                            product.id, 
                            product.our_price, 
                            scraped_data
                        )
                        
                        if anomalies:
                            anomaly_count += len(anomalies)
                            logger.warning(f"âš ï¸ {len(anomalies)} anomali tespit edildi: {product.name}")
                            for anomaly in anomalies:
                                logger.warning(
                                    f"   {anomaly['type']} - {anomaly['severity']}: "
                                    f"Bizim: â‚º{anomaly['our_price']:.2f}, "
                                    f"Piyasa: â‚º{anomaly['market_avg']:.2f}, "
                                    f"Fark: %{anomaly['deviation']:.1f}"
                                )
                    
                    # SaygÄ±lÄ± gecikme
                    time.sleep(random.uniform(3, 7))
                    
                except Exception as e:
                    logger.error(f"ÃœrÃ¼n iÅŸleme hatasÄ± {product.name}: {e}")
                    continue
            
            logger.info(f"ğŸ‰ Scraping tamamlandÄ±!")
            logger.info(f"âœ… {scraped_count}/{len(products)} Ã¼rÃ¼n iÅŸlendi")
            logger.info(f"âš ï¸ {anomaly_count} anomali tespit edildi")
            
            return {
                'total_products': len(products),
                'scraped_products': scraped_count,
                'anomalies_detected': anomaly_count
            }
            
        except Exception as e:
            logger.error(f"Toplu scraping hatasÄ±: {e}")
            return None

def main():
    """Test Ã§alÄ±ÅŸtÄ±rmasÄ±"""
    scraper = AkakceScraper()
    
    # Tek Ã¼rÃ¼n testi
    test_product = "iPhone 15 128GB"
    result = scraper.scrape_product_data(test_product)
    
    if result:
        print(f"\nğŸ‰ Test BaÅŸarÄ±lÄ±: {test_product}")
        print(f"ğŸ’° Ortalama Fiyat: â‚º{result['avg_price']:.2f}")
        print(f"ğŸ“Š Fiyat AralÄ±ÄŸÄ±: â‚º{result['min_price']:.2f} - â‚º{result['max_price']:.2f}")
        print(f"ğŸª SatÄ±cÄ± SayÄ±sÄ±: {result['price_count']}")
    else:
        print("âŒ Test baÅŸarÄ±sÄ±z")

if __name__ == "__main__":
    main()